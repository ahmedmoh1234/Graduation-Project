{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------IMPORTS----------------------------\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------READING DATASET LABELS----------------------\n",
    "\n",
    "images_path = \"D:\\Work\\CCE\\Year 4 - Senior 2\\Semester 1\\Senior-2-Semester-1\\GP\\Graduation-Project\\Clothes Description Module\\Datasets\\Dataset 1\\Images\"\n",
    "csv_path = \"D:\\Work\\CCE\\Year 4 - Senior 2\\Semester 1\\Senior-2-Semester-1\\GP\\Graduation-Project\\Clothes Description Module\\Datasets\\Dataset 1\\CSV\\images.csv\"\n",
    "df = pd.read_csv(csv_path) # Reading the csv file into a dataframe using pandas\n",
    "df.head() # Displaying the first 5 rows of the dataframe\n",
    "\n",
    "# # Removing all entries with label = Not sure or label Other\n",
    "df = df[df['label'] != 'Not sure']\n",
    "df = df[df['label'] != 'Other']\n",
    "\n",
    "#Remove colums: sender_id and kids\n",
    "df = df.drop(['sender_id', 'kids'], axis=1)\n",
    "df.head() # Displaying the first 5 rows of the dataframe\n",
    "\n",
    "# Extract Unique Labels = Number of classes but remove the 'Not sure' and 'Other' labels\n",
    "class_names = df['label'].unique()\n",
    "num_classes = len(class_names)\n",
    "class_ids = dict(zip(class_names, range(num_classes))) # Make a dictionary of class names and their corresponding ids\n",
    "\n",
    "print(f'There are {num_classes} classes: {class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------READING IMAGES AND THEIR LABELS----------------------\n",
    "images = []\n",
    "images_labels = []\n",
    "\n",
    "#Read all the images in the file with the path images_path\n",
    "for filename in os.listdir(images_path):\n",
    "    image_path = os.path.join(images_path, filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    filename = filename.split('.')[0]\n",
    "    label = df.loc[df['image'] == filename]['label']\n",
    "    if(label.empty or image is None):\n",
    "        continue\n",
    "    label = label.values[0]\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    images.append(image)\n",
    "    images_labels.append(class_ids[label])\n",
    "        \n",
    "# Convert the images and labels to numpy arrays        \n",
    "images = np.array(images, dtype=np.float32)\n",
    "images_labels = np.array(images_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the images\n",
    "images = images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------DATA SPLITTING----------------------------------------\n",
    "\n",
    "# Split the dataset into 70% for training and 15% for testing and 15% for cross-validation\n",
    "trainval_images, x_test, trainval_labels, y_test = train_test_split(images, images_labels, test_size=0.15, random_state=42)\n",
    "\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(trainval_images, trainval_labels, test_size=0.15/0.85, random_state=42)\n",
    "\n",
    "m_train = len(x_train)\n",
    "m_test = len(x_test)\n",
    "m_cv = len(x_cv)\n",
    "\n",
    "# Print the size of each set\n",
    "print(f\"Dataset Size: {images.shape[0]}\")\n",
    "print(f\"Training set size: {m_train} images\")\n",
    "print(f\"Cross-validation set size: {m_cv} images\")\n",
    "print(f\"Testing set size: {m_test} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Data Augmentation on x_train and y_train using tensorflow ImageDataGenerator\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# Create a generator for the training set\n",
    "train_generator = train_datagen.flow(x_train, y_train)\n",
    "\n",
    "#use the train_generator\n",
    "x_train, y_train = train_generator.next()\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {len(x_train)}\")\n",
    "print(f\"the shape of the training set (target) is: {len(y_train)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------BUILD MODEL USING TRANSFER LEARNING----------------------------------------\n",
    "\n",
    "# Use ResNet pretrained model\n",
    "# Load the pre-trained ResNet50 model without the top layer (i.e., the fully connected layers)\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze the pre-trained layers to avoid changing their weights during training\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a new fully connected layer for the specific classification task\n",
    "x = Flatten()(resnet_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(num_classes, activation='linear')(x)\n",
    "\n",
    "# Create a new model by combining the pre-trained ResNet50 model with the new fully connected layers\n",
    "model = Model(inputs=resnet_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------BUILD MODEL USING CNN----------------------------------------\n",
    "\n",
    "# Create a convolutional neural network (CNN)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape = (128, 128, 3),kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'Conv1'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2, name = 'Max1'),\n",
    "    tf.keras.layers.Conv2D(64, (5, 5), padding = 'same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'Conv2'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2, name = 'Max2'),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'Conv3'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2, name = 'Max3'),\n",
    "    tf.keras.layers.Conv2D(512, (5, 5), strides = 2, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'Conv4'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2, name = 'Max4'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'Dense1'),    \n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'Dense2'),    \n",
    "    tf.keras.layers.Dense(len(class_names), activation='linear', name = 'Dense3')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------TRAINING----------------------------------------\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "# the training process will stop if the validation loss does not improve after 3 epochs.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "# his callback can be used to save the best model weights during training based on a given metric (e.g., validation accuracy or loss).\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=30, validation_data=(x_cv, y_cv),callbacks=[early_stopping,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------DELETE MODEL----------------------------------\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------SAVE MODEL-----------------------------------\n",
    "pickle.dump(model, open('clothing_detector.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------PREDICTIONS----------------------------------------\n",
    "train_predictions = model.predict(x_train)\n",
    "cv_predictions = model.predict(x_cv)\n",
    "test_predictions = model.predict(x_test)\n",
    "\n",
    "train_count_correct = 0\n",
    "cv_count_correct = 0\n",
    "test_count_correct = 0\n",
    "\n",
    "for i in range(m_train):\n",
    "    predicted = np.argmax(train_predictions[i])\n",
    "    if (predicted == y_train[i]):\n",
    "        train_count_correct += 1\n",
    "train_accuracy = train_count_correct / m_train\n",
    "\n",
    "for i in range(m_test):\n",
    "    predicted = np.argmax(test_predictions[i])\n",
    "    if (predicted == y_test[i]):\n",
    "        test_count_correct += 1\n",
    "test_accuracy = test_count_correct / m_test\n",
    "\n",
    "for i in range(m_cv):\n",
    "    predicted = np.argmax(cv_predictions[i])\n",
    "    if (predicted == y_cv[i]):\n",
    "        cv_count_correct += 1\n",
    "cv_accuracy = cv_count_correct / m_cv\n",
    "\n",
    "print(\"Train Accuracy = %.2f\" % (train_accuracy*100),'% with', m_train, 'training examples')\n",
    "print(\"CV Accuracy = %.2f\" % (cv_accuracy*100),'% with', m_cv, 'test examples')\n",
    "print(\"Test Accuracy = %.2f\" % (test_accuracy*100),'% with', m_test, 'test examples')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3 (tags/v3.9.3:e723086, Apr  2 2021, 11:35:20) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "104451bbd5af0301cd2444e0cd70e99d9901e7de2f5bbc69a1b6555b99a1a266"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
