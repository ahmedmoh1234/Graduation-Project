{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from skimage.color import rgb2gray\n",
    "import pickle\n",
    "import os\n",
    "dirname = os.path.abspath('')\n",
    "datasets_path = os.path.join(dirname, 'Datasets')\n",
    "olivetti_path = os.path.join(datasets_path, 'Olivetti')\n",
    "parameters_path = os.path.join(dirname, 'PCA Parameters\\Olivetti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- LOAD OLIVIETTA DATASET ---------------------------- #\n",
    "\n",
    "lfw_people = fetch_olivetti_faces(data_home=olivetti_path, shuffle=True, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (400, 64, 64)\n",
      "Each image has size: 64 x 64\n",
      "----------------------------------------------------------------------\n",
      "N^2 = n_features = h x w = 4096\n",
      "----------------------------------------------------------------------\n",
      "y has shape: (400,)\n",
      "Number of people = 40\n"
     ]
    }
   ],
   "source": [
    "images = lfw_people.images\n",
    "print('Images shape:',images.shape)\n",
    "\n",
    "total_images, height, width = images.shape\n",
    "print(f'Each image has size: {height} x {width}')\n",
    "\n",
    "print(70* '-')\n",
    "\n",
    "n_features = height*width\n",
    "print(f'N^2 = n_features = h x w = {n_features}')\n",
    "\n",
    "print(70* '-')\n",
    "\n",
    "y = lfw_people.target\n",
    "print('y has shape:', y.shape)\n",
    "\n",
    "num_people = np.max(y) + 1\n",
    "print('Number of people =', num_people)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- CREATE PEOPLE DICTIONARY -----------------------#\n",
    "\n",
    "# Key: person ID\n",
    "# Value: List of all person images indices\n",
    "person_image_dict = dict()\n",
    "\n",
    "for image_index in range(total_images):\n",
    "    if (y[image_index] not in person_image_dict.keys()):\n",
    "        person_image_dict[y[image_index]] = [image_index]\n",
    "    else:\n",
    "        person_image_dict[y[image_index]].append(image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (320, 64, 64)\n",
      "Test images shape: (80, 64, 64)\n",
      "----------------------------------------------------------------------\n",
      "Number of train images: 320\n",
      "Number of test images: 80\n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "test_images = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "# guarantee that each person has at least 1 image in test set\n",
    "for image_index_lst in person_image_dict.values():\n",
    "    total_size = len(image_index_lst)\n",
    "    size_train = int(0.8 * total_size)\n",
    "    for index in range(len(image_index_lst)):\n",
    "        image = images[image_index_lst[index]]\n",
    "        label = y[image_index_lst[index]]\n",
    "        if (index < size_train):\n",
    "            train_images.append(image)\n",
    "            y_train.append(label)\n",
    "        else:\n",
    "            test_images.append(image)\n",
    "            y_test.append(label)\n",
    "    \n",
    "\n",
    "train_images = np.array(train_images)\n",
    "test_images = np.array(test_images)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print('Train images shape:', train_images.shape)\n",
    "print('Test images shape:', test_images.shape)\n",
    "\n",
    "m = train_images.shape[0]\n",
    "m_test = test_images.shape[0]\n",
    "print(70* '-')\n",
    "print('Number of train images:', m)\n",
    "print('Number of test images:', m_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (400, 64, 64)\n",
      "Each image has size: 64 x 64\n",
      "----------------------------------------------------------------------\n",
      "M = Number of examples =  320\n",
      "M_test = Number of test examples =  80\n",
      "Train images shape: (320, 64, 64)\n",
      "Test images shape: (80, 64, 64)\n",
      "N^2 = n_features = h x w = 4096\n",
      "----------------------------------------------------------------------\n",
      "y has shape: (400,)\n",
      "y_train has shape: (320,)\n",
      "y_test has shape: (80,)\n",
      "Number of people = 40\n"
     ]
    }
   ],
   "source": [
    "images = lfw_people.images\n",
    "print('Images shape:',images.shape)\n",
    "\n",
    "total_images, height, width = images.shape\n",
    "print(f'Each image has size: {height} x {width}')\n",
    "\n",
    "print(70* '-')\n",
    "\n",
    "m = int(total_images * 0.8)\n",
    "print('M = Number of examples = ', m)\n",
    "\n",
    "m_test = total_images - m\n",
    "print('M_test = Number of test examples = ', m_test)\n",
    "\n",
    "train_images = images[:int(m)]\n",
    "print('Train images shape:', train_images.shape)\n",
    "\n",
    "test_images = images[int(m):]\n",
    "print('Test images shape:', test_images.shape)\n",
    "\n",
    "n_features = height*width\n",
    "print(f'N^2 = n_features = h x w = {n_features}')\n",
    "\n",
    "print(70* '-')\n",
    "\n",
    "y = lfw_people.target\n",
    "print('y has shape:', y.shape)\n",
    "\n",
    "y_train = y[:int(m)]\n",
    "print('y_train has shape:', y_train.shape)\n",
    "\n",
    "y_test = y[int(m):]\n",
    "print('y_test has shape:', y_test.shape)\n",
    "\n",
    "# target_names = lfw_people.target_names\n",
    "num_people = np.max(y) + 1\n",
    "print('Number of people =', num_people)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    # X: (m x n) matrix, m: number of samples, n: number of features\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    normalized_X = (X - mu) / sigma\n",
    "    \n",
    "    return (normalized_X, mu, sigma)\n",
    "\n",
    "def pca(X):\n",
    "    # cov: (n x n)\n",
    "    # X: (m x n)\n",
    "    cov = np.matmul(X.T, X) / X.shape[0]\n",
    "\n",
    "    eigenvectors, eigenvalues, _ = np.linalg.svd(cov)\n",
    "\n",
    "    return eigenvectors, eigenvalues\n",
    "\n",
    "\n",
    "def projectData(X, U, K):\n",
    "    # X: (m x n)\n",
    "    # U: (n x n)\n",
    "    # K: scalar\n",
    "    # Z: (m x K)\n",
    "    Z = np.matmul(X, U[:, :K])\n",
    "    return Z\n",
    "\n",
    "def extract_pca_features(images, load=False, num_pca_components=0.95):\n",
    "    if (load):\n",
    "        U = pickle.load(open(parameters_path + '\\Olivetti Eigenvectors.pkl', 'rb'))\n",
    "        K = pickle.load(open(parameters_path + '\\Olivetti K.pkl', 'rb'))\n",
    "        mu = pickle.load(open(parameters_path + '\\Olivetti mu.pkl', 'rb'))\n",
    "        sigma = pickle.load(open(parameters_path + '\\Olivetti sigma.pkl', 'rb'))\n",
    "        X = images.reshape(images.shape[0], -1)\n",
    "        X = (X - mu) / sigma\n",
    "        Z = projectData(X, U, K)\n",
    "        Z = projectData(X, U, K)\n",
    "        return Z\n",
    "    else:\n",
    "        X = images.reshape(images.shape[0], -1)\n",
    "        X, mu, sigma = featureNormalize(X)\n",
    "        U, S = pca(X)\n",
    "        K = 0\n",
    "        for i in range(len(S)):\n",
    "            if (np.sum(S[:i]) / np.sum(S)) >= num_pca_components:\n",
    "                K = i\n",
    "                break\n",
    "        print('K =', K)\n",
    "        \n",
    "        Z = projectData(X, U, K)\n",
    "        \n",
    "        pickle.dump(U, open(parameters_path + '\\Olivetti Eigenvectors.pkl', 'wb'))\n",
    "        pickle.dump(K, open(parameters_path + '\\Olivetti K.pkl', 'wb'))\n",
    "        pickle.dump(mu, open(parameters_path + '\\Olivetti mu.pkl', 'wb'))\n",
    "        pickle.dump(sigma, open(parameters_path + '\\Olivetti sigma.pkl', 'wb'))\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 112\n",
      "PCA Features Train: (320, 112)\n",
      "PCA Features Test: (80, 112)\n"
     ]
    }
   ],
   "source": [
    "pca_features_train = extract_pca_features(train_images,load=False, num_pca_components=0.95)\n",
    "pca_features_test = extract_pca_features(test_images,load=True, num_pca_components=0.95)\n",
    "print(f'PCA Features Train: {pca_features_train.shape}')\n",
    "print(f'PCA Features Test: {pca_features_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(x1, x2):\n",
    "    distance = np.linalg.norm(x1-x2)\n",
    "    return distance\n",
    "\n",
    "def KNN(test_point, training_features, labels, k):\n",
    "    distances = [calculateDistance(test_point, p) for p in training_features]\n",
    "    \n",
    "    #k_nearest: numpy array of size k holding indices of k nearest images\n",
    "    k_nearest = np.argpartition(distances, k)[:k]\n",
    "    # print(k_nearest)\n",
    "    \n",
    "    votes = np.zeros(num_people)\n",
    "\n",
    "    for i in k_nearest:\n",
    "        # print(i)\n",
    "        # print(labels[i])\n",
    "        # print('---')\n",
    "        votes[labels[i]] += 1\n",
    "\n",
    "    # print('---------------\\n',votes)\n",
    "    classification = np.argmax(votes)\n",
    "\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index in range(m_test):\n",
    "    # Get image\n",
    "    test_point = pca_features_test[index]\n",
    "\n",
    "    classification = KNN(test_point, pca_features_train, y_train, 5)\n",
    "    \n",
    "    if(classification == y_test[index]):\n",
    "        count += 1\n",
    "    \n",
    "print(f'Accuracy: {count/m_test*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
